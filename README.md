# RAG Demo using Couchbase, Streamlit, LangChain, and Ollama

This is a demo app built to chat with your custom PDFs using the vector search capabilities of Couchbase to augment the Ollama LLM results in a Retrieval-Augmented-Generation (RAG) model.

## Key Features

- **Local LLM with Ollama**: Uses Llama 3.2 for both RAG and pure LLM responses - completely free and runs locally
- **Async Embedding Generation**: Leverages Couchbase Eventing Service to generate document embeddings asynchronously
- **LLM Response Caching**: Uses [CouchbaseCache](https://python.langchain.com/v0.2/docs/integrations/llm_caching/#couchbase-cache) to avoid repeated LLM calls, saving time and resources
- **Dual Response System**: Compare RAG vs pure LLM answers side-by-side

> **Note**: You need Couchbase Server 7.6 or higher for Vector Search.

## Architecture

### How does it work?

You can upload your PDFs with custom data & ask questions about the data in the chat box.

For each question, you will get two answers:

- One using **RAG** (Couchbase logo) - uses context from your PDF
- One using **pure LLM** (ðŸ¤–) - Ollama without context

### Document Processing Flow

1. **PDF Upload**: Document is chunked into ~1500 character segments with 150 character overlap
2. **Storage**: Chunks are stored in Couchbase *without* embeddings
3. **Eventing Service**: Automatically detects new documents and generates embeddings asynchronously
4. **Query Time**: User questions are embedded via HuggingFace API, then vector search finds relevant chunks
5. **RAG**: Retrieved context is passed to Ollama Llama 3.2 for answer generation

### Embedding Strategy

- **Document Embeddings**: Generated by Couchbase Eventing Service (server-side, asynchronous)
- **Query Embeddings**: Generated by HuggingFace API (client-side, real-time)
- **Model**: `BAAI/bge-base-en-v1.5` for both document and query embeddings

## Prerequisites

- Python 3.8+
- Couchbase Server 7.6+ or Couchbase Capella
- [Ollama](https://ollama.com/) installed locally with `llama3.2` model pulled
- HuggingFace API token (free tier works)

## Setup Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Install and Setup Ollama

```bash
# Install Ollama (macOS/Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Pull Llama 3.2 model
ollama pull llama3.2

# Verify installation
ollama list
```

### 3. Configure Environment Variables

Copy the `secrets.example.toml` file in `.streamlit` folder and rename it to `secrets.toml`:

```toml
HUGGINGFACEHUB_API_TOKEN = "<your_huggingface_api_token>"
DB_CONN_STR = "<connection_string_for_couchbase_cluster>"
DB_USERNAME = "<username_for_couchbase_cluster>"
DB_PASSWORD = "<password_for_couchbase_cluster>"
DB_BUCKET = "<name_of_bucket_to_store_documents>"
DB_SCOPE = "<name_of_scope_to_store_documents>"
DB_COLLECTION = "<name_of_collection_to_store_documents>"
CACHE_COLLECTION = "<name_of_collection_to_cache_llm_responses>"
INDEX_NAME = "<name_of_fts_index_with_vector_support>"
AUTH_ENABLED = "False"  # Set to "True" to enable password protection
LOGIN_PASSWORD = "<password_to_access_streamlit_app>"  # Only needed if AUTH_ENABLED=True
```

### 4. Create the Search Index

We need to create the Search Index on the Full Text Service in Couchbase.

#### For Couchbase Capella

- [Import Search Index Instructions](https://docs.couchbase.com/cloud/search/import-search-index.html)
- Copy the index definition below to a new file `index.json`
- Import the file in Capella
- Click on Create Index

#### For Couchbase Server

- [Import Search Index Instructions](https://docs.couchbase.com/server/current/search/import-search-index.html)
- Navigate to Search â†’ Add Index â†’ Import
- Copy the index definition below
- Click on Create Index

#### Index Definition

This index is configured for the `BAAI/bge-base-en-v1.5` model with **768 dimensions**. Update the bucket, scope, and collection names to match your configuration.

```json
{
  "type": "fulltext-index",
  "name": "vector_idx",
  "sourceType": "gocbcore",
  "sourceName": "rag_demo",
  "planParams": {
    "maxPartitionsPerPIndex": 8,
    "indexPartitions": 16
  },
  "params": {
    "doc_config": {
      "docid_prefix_delim": "",
      "docid_regexp": "",
      "mode": "scope.collection.type_field",
      "type_field": "type"
    },
    "mapping": {
      "default_analyzer": "standard",
      "default_datetime_parser": "dateTimeOptional",
      "default_field": "_all",
      "default_mapping": {
        "dynamic": true,
        "enabled": false
      },
      "default_type": "_default",
      "docvalues_dynamic": false,
      "index_dynamic": true,
      "store_dynamic": false,
      "type_field": "_type",
      "types": {
        "pdf_app.documents": {
          "dynamic": false,
          "enabled": true,
          "properties": {
            "embedding": {
              "enabled": true,
              "dynamic": false,
              "fields": [
                {
                  "name": "embedding",
                  "type": "vector",
                  "dims": 768,
                  "similarity": "dot_product",
                  "index": true
                }
              ]
            },
            "page_content": {
              "enabled": true,
              "dynamic": false,
              "fields": [
                {
                  "name": "page_content",
                  "type": "text",
                  "index": true,
                  "store": true
                }
              ]
            },
            "metadata": {
              "enabled": true,
              "dynamic": true
            }
          }
        }
      }
    },
    "store": {
      "indexType": "scorch",
      "segmentVersion": 16
    }
  },
  "sourceParams": {}
}
```

**Important Configuration**:
- **Index Name**: `vector_idx`
- **Bucket**: `rag_demo`
- **Scope**: `pdf_app`
- **Collection**: `documents`
- **Vector Field**: `embedding` (768 dimensions)
- **Text Field**: `page_content`

Replace these values with your own bucket/scope/collection names in both the index definition and your `secrets.toml` file.

### 5. Setup Couchbase Eventing Function

Create an Eventing Function to automatically generate embeddings for uploaded documents.

#### Step-by-Step Eventing Setup

1. **Navigate to Eventing** in Couchbase Console
2. **Create New Function** and configure:
   - **Function Name**: `generate_pdf_embeddings`
   - **Listen to**: `rag_demo.pdf_app.documents` (your bucket.scope.collection)
   - **Settings**: 
     - Worker Count: 1
     - Deployment Feed Boundary: From now

3. **Configure Bindings**:

   **Bucket Binding** (to write embeddings back):
   - **Alias Name**: `dst_bucket`
   - **Bucket**: `rag_demo`
   - **Scope**: `pdf_app`
   - **Collection**: `documents`
   - **Permission**: Read and Write

   **URL Binding** (for HuggingFace API):
   - **Alias Name**: `hfApi`
   - **URL**: `https://api-inference.huggingface.co/pipeline/feature-extraction/BAAI/bge-base-en-v1.5`
   - **Authentication**: Bearer
   - **Bearer Token**: `<YOUR_HUGGINGFACE_API_TOKEN>`
   - **Allow Cookies**: âœ“
   - **Validate SSL Certificate**: âœ“

4. **Add the Function Code**:

```javascript
// Eventing Function for PDF Chat App with LangChain
// This function automatically generates embeddings for PDF chunks stored by LangChain

function OnUpdate(doc, meta) {
    // LangChain stores documents with 'page_content' field
    // Skip if no text content or if embedding already exists
    if (!doc.page_content || (doc.embedding && Array.isArray(doc.embedding) && doc.embedding.length > 0)) {
        return;
    }
    
    try {
        var textToEmbed = doc.page_content;
        
        // Call HuggingFace API to generate embeddings
        var body = { "inputs": textToEmbed };
        var response = curl("POST", hfApi, {
            body: JSON.stringify(body),
            headers: { 
                "Content-Type": "application/json"
            }
        });
        
        if (response.status !== 200) {
            log("HuggingFace API error for " + meta.id + ": Status " + response.status);
            return;
        }
        
        var respBody = String(response.body).trim();
        var embedding = null;
        
        // Parse the embedding from HuggingFace response
        try {
            var parsed = JSON.parse(respBody);
            if (Array.isArray(parsed)) {
                // HuggingFace returns array of arrays, we want the first one
                embedding = Array.isArray(parsed[0]) ? parsed[0] : parsed;
            }
        } catch (parseError) {
            // Fallback: Extract all float values using regex
            var matches = respBody.match(/-?\d+(\.\d+)?/g);
            if (matches) {
                embedding = matches.map(function(v) { return parseFloat(v); });
            }
        }
        
        // Store the embedding in the document
        if (embedding && embedding.length > 0) {
            doc.embedding = embedding;
            dst_bucket[meta.id] = doc;
            log("Successfully generated embedding for document: " + meta.id + " (dimension: " + embedding.length + ")");
        } else {
            log("Failed to extract valid embedding for document: " + meta.id);
        }
        
    } catch (err) {
        log("Embedding generation error for " + meta.id + ": " + err.toString());
    }
}

function OnDelete(meta, options) {
    log("Document deleted/expired: " + meta.id);
}
```

5. **Deploy the Function**:
   - Click **Deploy**
   - Wait for the function to reach "Deployed" state
   - Monitor logs for any errors

#### Important Notes

- The function processes documents in parallel based on worker count
- HuggingFace Inference API has rate limits on the free tier
- For production with high volume, consider using a local embedding service
- Check Eventing logs if embeddings aren't being generated

### 6. Run the Application

```bash
streamlit run chat_with_pdf.py
```

The app will be available at `http://localhost:8501`

## Usage

1. **Upload PDF**: Use the sidebar to upload your PDF document
2. **Wait for Processing**: The Eventing function will generate embeddings (may take a few seconds)
3. **Ask Questions**: Type your question in the chat input
4. **Compare Answers**: View both RAG (with context) and pure LLM responses

## Technology Stack

- **Frontend**: Streamlit
- **Vector Database**: Couchbase Vector Search
- **LLM Framework**: LangChain
- **Embeddings**: HuggingFace (`BAAI/bge-base-en-v1.5`)
- **LLM**: Ollama (Llama 3.2)
- **Document Processing**: PyPDF, RecursiveCharacterTextSplitter
- **Caching**: CouchbaseCache

